{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to import geometry msgs in rigid_transformations.py.\n",
      "WARNING:root:Failed to import ros dependencies in rigid_transforms.py\n",
      "WARNING:root:autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n",
      "WARNING:root:Unable to import pylibfreenect2. Python-only Kinect driver may not work properly.\n",
      "WARNING:root:Failed to import ROS in Kinect2_sensor.py. Kinect will not be able to be used in bridged mode\n",
      "WARNING:root:Unable to import Primsense sensor modules! Likely due to missing OpenNI2.\n",
      "WARNING:root:Unable to import pyrealsense2.\n",
      "WARNING:root:Failed to import ROS in ensenso_sensor.py. ROS functionality not available\n",
      "WARNING:root:Failed to import ROS in phoxi_sensor.py. PhoXiSensor functionality unavailable.\n",
      "WARNING:root:Unable to import generic sensor modules!.\n",
      "WARNING:root:Unable to import weight sensor modules!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from collections.abc import Iterable\n",
    "import robosuite\n",
    "import robosuite.utils.transform_utils as T\n",
    "from robosuite.wrappers import IKWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "from robosuite.utils.mjcf_utils import array_to_string, string_to_array\n",
    "from motion_planner import approach_to_object, move_to_6Dpos, get_camera_pos, force_gripper, move_to_pos_lrarm, move_to_pos, get_pos_and_rot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for getting a vertical image\n",
    "### arena_pos: pose of a bin\n",
    "### vis_on (default False): whether plotting the image or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertical_image(env, arena_pos, vis_on=False):\n",
    "    camera_id = env.sim.model.camera_name2id(\"eye_on_wrist\")\n",
    "    camera_obs = env.sim.render(\n",
    "        camera_name=env.camera_name,\n",
    "        width=env.camera_width,\n",
    "        height=env.camera_height,\n",
    "        depth=env.camera_depth\n",
    "    )\n",
    "    pnt_hem, rot_mat = get_camera_pos(arena_pos, np.array([0.0, 0.0, 0.0]))\n",
    "    env.sim.data.cam_xpos[camera_id] = pnt_hem\n",
    "    env.sim.data.cam_xmat[camera_id] = rot_mat.flatten()\n",
    "\n",
    "    camera_obs = env.sim.render(\n",
    "        camera_name=env.camera_name,\n",
    "        width=env.camera_width,\n",
    "        height=env.camera_height,\n",
    "        depth=env.camera_depth\n",
    "    )\n",
    "    if env.camera_depth:\n",
    "        vertical_color_image, ddd = camera_obs\n",
    "\n",
    "    extent = env.mjpy_model.stat.extent\n",
    "    near = env.mjpy_model.vis.map.znear * extent\n",
    "    far = env.mjpy_model.vis.map.zfar * extent\n",
    "\n",
    "    vertical_depth_image = near / (1 - ddd * (1 - near / far))\n",
    "    vertical_depth_image = np.where(vertical_depth_image > 0.25, vertical_depth_image, 1)\n",
    "\n",
    "    if vis_on:\n",
    "        plt.imshow(np.flip(vertical_color_image, axis=0))\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(np.flip(vertical_depth_image, axis=0), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    return np.flip(vertical_color_image, axis=0), np.flip(vertical_depth_image, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop objects in the bin sequentially\n",
    "### To ensure all objects placed in the bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_quat(rand=None):\n",
    "    if rand is None:\n",
    "        rand = np.random.rand(3)\n",
    "    else:\n",
    "        assert len(rand) == 3\n",
    "    r1 = np.sqrt(1.0 - rand[0])\n",
    "    r2 = np.sqrt(rand[0])\n",
    "    pi2 = np.pi * 2.0\n",
    "    t1 = pi2 * rand[1]\n",
    "    t2 = pi2 * rand[2]\n",
    "    return np.array(\n",
    "        (np.sin(t1) * r1, np.cos(t1) * r1, np.sin(t2) * r2, np.cos(t2) * r2),\n",
    "        dtype=np.float32,\n",
    "    )    \n",
    "\n",
    "def place_objects_in_bin(env, num_objects=10):\n",
    "    for i in range(num_objects):\n",
    "        object_z = env.env.model.bin_size[2] + 0.5\n",
    "        object_x = 0.0\n",
    "        object_y = 0.0\n",
    "        object_xyz = np.array([object_x, object_y, object_z])\n",
    "        pos = env.env.model.bin_offset + object_xyz\n",
    "        env.env.model.objects[i].set(\"pos\", array_to_string(pos))\n",
    "        quat = random_quat()\n",
    "        env.env.model.objects[i].set(\"quat\", array_to_string(quat))\n",
    "        env.reset_sims()\n",
    "        for j in range(0, 500):\n",
    "            env.env.sim.step()\n",
    "        for k in range(0, i + 1):\n",
    "            obj_str = env.env.item_names[k]\n",
    "            obj_id = env.env.obj_body_id[obj_str]\n",
    "            pos = env.env.sim.data.body_xpos[obj_id]\n",
    "            quat = env.env.sim.data.body_xquat[obj_id]\n",
    "            env.env.model.objects[k].set(\"pos\", array_to_string(pos))\n",
    "            env.env.model.objects[k].set(\"quat\", array_to_string(quat))\n",
    "    env.reset_sims()\n",
    "\n",
    "    bin_pos = env.env.model.bin_offset\n",
    "    for i in range(num_objects):\n",
    "        obj_str = env.env.item_names[i]\n",
    "        obj_id = env.env.obj_body_id[obj_str]\n",
    "        pos = env.env.sim.data.body_xpos[obj_id]\n",
    "        if abs(pos[0] - bin_pos[0]) < env.bin_size[0] / 2.0 and abs(pos[1] - bin_pos[1]) < env.bin_size[1] / 2.0:\n",
    "            continue\n",
    "        else:\n",
    "            object_z = env.env.model.bin_size[2] + 0.3\n",
    "            object_x = 0.1 * np.random.random() - 0.05\n",
    "            object_y = 0.1 * np.random.random() - 0.05\n",
    "            object_xyz = np.array([object_x, object_y, object_z])\n",
    "            pos = bin_pos + object_xyz\n",
    "            env.env.model.objects[i].set(\"pos\", array_to_string(pos))\n",
    "            env.reset_sims()\n",
    "            for j in range(0, 500):\n",
    "                env.env.sim.step()\n",
    "            for k in range(0, num_objects):\n",
    "                obj_str = env.env.item_names[k]\n",
    "                obj_id = env.env.obj_body_id[obj_str]\n",
    "                pos = env.env.sim.data.body_xpos[obj_id]\n",
    "                quat = env.env.sim.data.body_xquat[obj_id]\n",
    "                env.env.model.objects[k].set(\"pos\", array_to_string(pos))\n",
    "                env.env.model.objects[k].set(\"quat\", array_to_string(quat))\n",
    "        env.reset_sims()\n",
    "\n",
    "    item_list = []\n",
    "    pos_list = []\n",
    "    quat_list = []\n",
    "    for i in range(num_objects):\n",
    "        obj_str = env.item_names[i]\n",
    "        obj_id = env.obj_body_id[obj_str]\n",
    "        pos = env.sim.data.body_xpos[obj_id]\n",
    "        quat = env.sim.data.body_xquat[obj_id]\n",
    "        item_list.append(obj_str.split(\"_\")[1])\n",
    "        pos_list.append(pos)\n",
    "        quat_list.append(quat)\n",
    "    return item_list, pos_list, quat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters for the simulation\n",
    "### seed: random seed\n",
    "### num_objects: the number of graspable objects\n",
    "### num_episodes: the number of simulation episodes\n",
    "### num_steps: the number of steps for each epsiode\n",
    "### render : display all grasping procedures if it is true\n",
    "### bin_type: table or bin\n",
    "### object_type: T, L, or 3DNet meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "num_objects = 5\n",
    "num_episodes = 100\n",
    "num_steps = 5\n",
    "render = True\n",
    "bin_type = \"table\"\n",
    "object_type = \"T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MuJoCo baxter environements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "\u001b[35mFCGQCNNModelFactory \u001b[32mINFO    \u001b[0m \u001b[37mInitializing FC-GQ-CNN with Tensorflow as backend...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mBuilding Network...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mBuilding Image Stream...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mBuilding convolutional layer: conv1_1...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mBuilding convolutional layer: conv1_2...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mBuilding convolutional layer: conv2_1...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mBuilding convolutional layer: conv2_2...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mConverting fc layer fc3 to fully convolutional...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mConverting fc layer fc4 to fully convolutional...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mConverting fc layer fc5 to fully convolutional...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mBuilding Pair-wise Softmax Layer...\u001b[0m\n",
      "\u001b[35mFCGQCNNTF  \u001b[32mINFO    \u001b[0m \u001b[37mInitializing TF Session...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "env = robosuite.make(\n",
    "    \"BaxterSteepedBinCollectData\",\n",
    "    bin_type=bin_type,\n",
    "    object_type=object_type,\n",
    "    ignore_done=True,\n",
    "    has_renderer=True,\n",
    "    camera_name=\"eye_on_wrist\",\n",
    "    gripper_visualization=True,\n",
    "    use_camera_obs=False,\n",
    "    camera_depth=True,\n",
    "    num_objects=num_objects,\n",
    "    control_freq=100\n",
    ")\n",
    "env = IKWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cam_offset: differences of x, y and z coordinates between the end effector of Baxter and the camera\n",
    "## camera_id: to set camera pose as we want, we need to know id of the camera on the robot arm \n",
    "## arena_pos: pose of the bin containing objects\n",
    "## init_pos: initial pose of the end effector\n",
    "## release_pos: where to release objects after the success of grasps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8bfcf886d86e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcam_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15855\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcamera_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera_name2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eye_on_wrist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marena_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco_arena\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_abs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minit_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marena_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcam_offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "cam_offset = np.array([0.05, 0, 0.15855])\n",
    "camera_id = env.sim.model.camera_name2id(\"eye_on_wrist\")\n",
    "\n",
    "arena_pos = env.env.mujoco_arena.bin_abs\n",
    "init_pos = arena_pos + np.array([0, 0, 0.7]) - cam_offset\n",
    "release_pos = arena_pos + np.array([0, 0.59, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect grasping dataset\n",
    "### Move to the inital position\n",
    "### Sample one approaching vector\n",
    "### Get the image viewed at the approaching vector and find the best grasp using the pretrained gqcnn\n",
    "### Move to target point with hover distance\n",
    "### Move to real target position\n",
    "### Try the grasp\n",
    "### Check collision\n",
    "#### If success, go back to the first step\n",
    "#### If failure, move the robot arm to the release pos and release the holding object and go back to the first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_episodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d4737cfc2cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reset!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mitem_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquat_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplace_objects_in_bin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Place all objects in bin!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvt_c_im_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt_d_im_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot_c_im_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot_d_im_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_episodes' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_episodes):\n",
    "    print(\"Reset!\")\n",
    "    item_list, pos_list, quat_list = place_objects_in_bin(env, num_objects=env.num_objects)\n",
    "    print(\"Place all objects in bin!\")\n",
    "    vt_c_im_list, vt_d_im_list, rot_c_im_list, rot_d_im_list = [], [], [], []\n",
    "    vt_g_pos_list, vt_g_euler_list, rot_g_pos_list, rot_g_euler_list = [], [], [], []\n",
    "    g_label_list = []\n",
    "    success_count, failure_count, controller_failure = 0, 0, 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        ### Move to the inital position\n",
    "        stucked = move_to_pos_lrarm(env, np.array([0.4, 0.6, 1.0]), np.array([0.4, -0.6, 1.0]), level=1.0, render=render)\n",
    "        if stucked == -1:\n",
    "            controller_failure += 1\n",
    "            continue\n",
    "\n",
    "        ### Sample one approaching vector\n",
    "        vt_c_im, vt_d_im = get_vertical_image(env, arena_pos, vis_on=False)\n",
    "        phi = np.pi / 6.0 * np.random.random()\n",
    "        theta = 2 * np.pi * np.random.random()\n",
    "\n",
    "        camera_obs = env.sim.render(\n",
    "            camera_name=env.camera_name,\n",
    "            width=env.camera_width,\n",
    "            height=env.camera_height,\n",
    "            depth=env.camera_depth\n",
    "        )\n",
    "        pnt_hem, rot_mat = get_camera_pos(arena_pos, np.array([0, phi, theta]))\n",
    "        env.sim.data.cam_xpos[camera_id] = pnt_hem\n",
    "        env.sim.data.cam_xmat[camera_id] = rot_mat.flatten()\n",
    "\n",
    "        sel_start = time.time()\n",
    "\n",
    "        ### Run FCGQQCNN\n",
    "        print(\"Try FC-GQ-CNN\")\n",
    "        [result, depths, d_im], rot_c_im, rot_d_im = env.env.gqcnn(vis_on=False, num_candidates=10) ## vis_on \n",
    "\n",
    "        if isinstance(result, Iterable):\n",
    "            sample_grasp_idx = np.random.randint(10, size=1)\n",
    "            result = result[sample_grasp_idx[0]].grasp\n",
    "        else:\n",
    "            if result is None:\n",
    "                continue\n",
    "            else:\n",
    "                result = result.grasp\n",
    "\n",
    "        p_x, p_y = result.center\n",
    "        print('p_x, p_y: ', p_x, p_y)\n",
    "\n",
    "        graspZ = result.depth\n",
    "        print('graspZ: ', graspZ)\n",
    "\n",
    "        dx, dy = env.env._pixel2pos(p_x, p_y, graspZ, arena_pos=[0, 0, 0])\n",
    "        dz = graspZ\n",
    "\n",
    "        # Move to target point with hover distance\n",
    "        t_pos = pnt_hem\n",
    "        t_angle = [result.angle, phi, theta]\n",
    "\n",
    "        stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.10], grasp=0.0, level=4.0, render=render)\n",
    "        if stucked == -1:\n",
    "            controller_failure += 1\n",
    "            continue\n",
    "        print('arrived at the hover point')\n",
    "\n",
    "        vt_c_im_list.append(vt_c_im)\n",
    "        vt_d_im_list.append(vt_d_im)\n",
    "        rot_c_im_list.append(rot_c_im)\n",
    "        rot_d_im_list.append(rot_d_im)\n",
    "        vt_g_pos_list.append(t_pos)\n",
    "        vt_g_euler_list.append(t_angle)\n",
    "        rot_g_pos_list.append([dx, dy, dz])\n",
    "        rot_g_euler_list.append([result.angle, 0, 0])\n",
    "\n",
    "        # Move to real target position\n",
    "        stucked = approach_to_object(env, t_pos, t_angle, cam_offset=[-dx, dy, dz + 0.01], grasp=0.0, level=2.0, render=render)\n",
    "        if stucked == -1:\n",
    "            failure_count += 1\n",
    "            g_label_list.append(0.0)\n",
    "            print('Grasping Failed, Success rate: ', success_count / (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                ', total trial: ', success_count, failure_count, controller_failure)\n",
    "            stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.10], grasp=0.0, level=1.0, render=render)\n",
    "            continue\n",
    "\n",
    "        # Grasping\n",
    "        obs = env.env._get_observation()\n",
    "        print('arrived at:', obs['right_eef_pos'], '(before grasping)')\n",
    "\n",
    "        stucked = force_gripper(env, grasp=1.0, render=render)\n",
    "        print(\"Try grasping!\")\n",
    "        stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.30], grasp=1.0, level=0.1, render=render)\n",
    "        print(\"Go back to the hover point.\")\n",
    "\n",
    "        # Check collision - success/failure\n",
    "        collision = env._check_contact()\n",
    "        if collision:\n",
    "            success_count += 1\n",
    "            g_label_list.append(1.0)\n",
    "            print('Grasping Success, Success rate: ', success_count / (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                ', total trial: ', success_count, failure_count, controller_failure)\n",
    "            move_to_pos(env, release_pos, 0.0, 1.0, level=0.1, render=render)\n",
    "            force_gripper(env, grasp=0.0, render=render)\n",
    "        else:\n",
    "            failure_count += 1\n",
    "            g_label_list.append(0.0)\n",
    "            print('Grasping Failed, Success rate: ', success_count/ (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                ', total trial: ', success_count, failure_count, controller_failure)\n",
    "\n",
    "    env.env.reset_objects()\n",
    "    save_file_name = \"data/6DoF_grasp/episode_\" + str(i) + \".npz\"\n",
    "    np.savez(save_file_name, item_list, pos_list, quat_list, vt_c_im_list, vt_d_im_list, rot_c_im_list, rot_d_im_list, \n",
    "        vt_g_pos_list, vt_g_euler_list, rot_g_pos_list, rot_g_euler_list, g_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
