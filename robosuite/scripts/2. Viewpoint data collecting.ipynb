{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this on uses the CPUExtensionBuilder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0406 10:02:34.125411 140561832568128 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from collections.abc import Iterable\n",
    "import robosuite\n",
    "import robosuite.utils.transform_utils as T\n",
    "from robosuite.wrappers import IKWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "from robosuite.utils.mjcf_utils import array_to_string, string_to_array\n",
    "from motion_planner import approach_to_object, move_to_6Dpos, get_camera_pos, force_gripper, move_to_pos_lrarm, move_to_pos, get_pos_and_rot\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from grasp_network import VPPNET\n",
    "import json\n",
    "\n",
    "INIT_ARM_POS = [0.40933302, -1.24377906, 0.68787495, 2.03907987, -0.27229507, 0.8635629,\n",
    "                0.46484251, 0.12655639, -0.74606415, -0.15337326, 2.04313409, 0.39049096,\n",
    "                0.30120114, 0.43309788]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertical_image(env, arena_pos, vis_on=False):\n",
    "    camera_id = env.sim.model.camera_name2id(\"eye_on_wrist\")\n",
    "    camera_obs = env.sim.render(\n",
    "        camera_name=env.camera_name,\n",
    "        width=env.camera_width,\n",
    "        height=env.camera_height,\n",
    "        depth=env.camera_depth\n",
    "    )\n",
    "    pnt_hem, rot_mat = get_camera_pos(arena_pos, np.array([0.0, 0.0, 0.0]))\n",
    "    env.sim.data.cam_xpos[camera_id] = pnt_hem\n",
    "    env.sim.data.cam_xmat[camera_id] = rot_mat.flatten()\n",
    "\n",
    "    camera_obs = env.sim.render(\n",
    "        camera_name=env.camera_name,\n",
    "        width=env.camera_width,\n",
    "        height=env.camera_height,\n",
    "        depth=env.camera_depth\n",
    "    )\n",
    "    if env.camera_depth:\n",
    "        vertical_color_image, ddd = camera_obs\n",
    "\n",
    "    extent = env.mjpy_model.stat.extent\n",
    "    near = env.mjpy_model.vis.map.znear * extent\n",
    "    far = env.mjpy_model.vis.map.zfar * extent\n",
    "\n",
    "    vertical_depth_image = near / (1 - ddd * (1 - near / far))\n",
    "    vertical_depth_image = np.where(vertical_depth_image > 0.25, vertical_depth_image, 1)\n",
    "\n",
    "    if vis_on:\n",
    "        plt.imshow(np.flip(vertical_color_image, axis=0))\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(np.flip(vertical_depth_image, axis=0), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    return np.flip(vertical_color_image, axis=0), np.flip(vertical_depth_image, axis=0)\n",
    "\n",
    "def random_quat(rand=None):\n",
    "    if rand is None:\n",
    "        rand = np.random.rand(3)\n",
    "    else:\n",
    "        assert len(rand) == 3\n",
    "    r1 = np.sqrt(1.0 - rand[0])\n",
    "    r2 = np.sqrt(rand[0])\n",
    "    pi2 = np.pi * 2.0\n",
    "    t1 = pi2 * rand[1]\n",
    "    t2 = pi2 * rand[2]\n",
    "    return np.array(\n",
    "        (np.sin(t1) * r1, np.cos(t1) * r1, np.sin(t2) * r2, np.cos(t2) * r2),\n",
    "        dtype=np.float32,\n",
    "    )    \n",
    "\n",
    "def place_objects_in_bin(env, num_objects=10):\n",
    "\n",
    "    for i in range(num_objects):\n",
    "        object_z = env.env.model.bin_size[2] + 0.5\n",
    "        object_x = 0.0\n",
    "        object_y = 0.0\n",
    "        object_xyz = np.array([object_x, object_y, object_z])\n",
    "        pos = env.env.model.bin_offset + object_xyz\n",
    "        env.env.model.objects[i].set(\"pos\", array_to_string(pos))\n",
    "        quat = random_quat()\n",
    "        env.env.model.objects[i].set(\"quat\", array_to_string(quat))\n",
    "        env.reset_sims()\n",
    "        for j in range(0, 500):\n",
    "            env.env.sim.step()\n",
    "        for k in range(0, i + 1):\n",
    "            obj_str = env.env.item_names[k]\n",
    "            obj_id = env.env.obj_body_id[obj_str]\n",
    "            pos = env.env.sim.data.body_xpos[obj_id]\n",
    "            quat = env.env.sim.data.body_xquat[obj_id]\n",
    "            env.env.model.objects[k].set(\"pos\", array_to_string(pos))\n",
    "            env.env.model.objects[k].set(\"quat\", array_to_string(quat))\n",
    "    env.reset_sims()\n",
    "\n",
    "    bin_pos = env.env.model.bin_offset\n",
    "    for i in range(num_objects):\n",
    "        obj_str = env.env.item_names[i]\n",
    "        obj_id = env.env.obj_body_id[obj_str]\n",
    "        pos = env.env.sim.data.body_xpos[obj_id]\n",
    "        if abs(pos[0] - bin_pos[0]) < env.bin_size[0] / 2.0 and abs(pos[1] - bin_pos[1]) < env.bin_size[1] / 2.0:\n",
    "            continue\n",
    "        else:\n",
    "            object_z = env.env.model.bin_size[2] + 0.3\n",
    "            object_x = 0.1 * np.random.random() - 0.05\n",
    "            object_y = 0.1 * np.random.random() - 0.05\n",
    "            object_xyz = np.array([object_x, object_y, object_z])\n",
    "            pos = bin_pos + object_xyz\n",
    "            env.env.model.objects[i].set(\"pos\", array_to_string(pos))\n",
    "            env.reset_sims()\n",
    "            for j in range(0, 500):\n",
    "                env.env.sim.step()\n",
    "            for k in range(0, num_objects):\n",
    "                obj_str = env.env.item_names[k]\n",
    "                obj_id = env.env.obj_body_id[obj_str]\n",
    "                pos = env.env.sim.data.body_xpos[obj_id]\n",
    "                quat = env.env.sim.data.body_xquat[obj_id]\n",
    "                env.env.model.objects[k].set(\"pos\", array_to_string(pos))\n",
    "                env.env.model.objects[k].set(\"quat\", array_to_string(quat))\n",
    "        env.reset_sims()\n",
    "\n",
    "    item_list = []\n",
    "    pos_list = []\n",
    "    quat_list = []\n",
    "    for i in range(num_objects):\n",
    "        obj_str = env.item_names[i]\n",
    "        obj_id = env.obj_body_id[obj_str]\n",
    "        pos = env.sim.data.body_xpos[obj_id]\n",
    "        quat = env.sim.data.body_xquat[obj_id]\n",
    "        item_list.append(obj_str.split(\"_\")[1])\n",
    "        pos_list.append(pos)\n",
    "        quat_list.append(quat)\n",
    "    return item_list, pos_list, quat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters for the simulation\n",
    "### seed: random seed\n",
    "### num_objects: the number of graspable objects\n",
    "### num_episodes: the number of simulation episodes\n",
    "### num_steps: the number of steps for each epsiode\n",
    "### render : display all grasping procedures if it is true\n",
    "### bin_type: table or bin\n",
    "### object_type: T, L, or 3DNet meshes\n",
    "### test: True: test the trained network, False: collect data\n",
    "### config_file: If test is True, path of the config file of the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "num_objects = 5\n",
    "num_episodes = 100\n",
    "num_steps = 5\n",
    "render = True\n",
    "bin_type = \"table\"\n",
    "object_type = \"smallcube\"\n",
    "test = False\n",
    "config_file = \"config_example.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MuJoCo baxter environements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "# env = robosuite.make(\n",
    "#     \"BaxterPush\",#\"BaxterSteepedBinCollectData\",\n",
    "#     bin_type=bin_type,\n",
    "#     object_type=object_type,\n",
    "#     ignore_done=True,\n",
    "#     has_renderer=True,\n",
    "#     camera_name=\"eye_on_wrist\",\n",
    "#     gripper_visualization=True,\n",
    "#     use_camera_obs=False,\n",
    "#     camera_depth=True,\n",
    "#     num_objects=num_objects,\n",
    "#     control_freq=100\n",
    "# )\n",
    "\n",
    "env = robosuite.make(\n",
    "        \"BaxterPush\",\n",
    "        bin_type='table',\n",
    "        object_type= object_type,\n",
    "        ignore_done=True,\n",
    "        has_renderer= True,\n",
    "#         has_offscreen_renderer= not bool(render), # added this new line\n",
    "        camera_name=\"eye_on_right_wrist\",\n",
    "        gripper_visualization=True,\n",
    "        use_camera_obs=False,\n",
    "#         use_object_obs=False,\n",
    "        camera_depth=True,\n",
    "        num_objects=1,\n",
    "        control_freq=100,\n",
    "        camera_width=192,\n",
    "        camera_height=192,\n",
    "        crop=128\n",
    "    )\n",
    "env = IKWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for getting the depth image and viewpoint selection (CEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im_depth(theta, phi):\n",
    "    camera_obs = env.sim.render(\n",
    "        camera_name=env.camera_name,\n",
    "        width=env.camera_width,\n",
    "        height=env.camera_height,\n",
    "        depth=env.camera_depth\n",
    "    )\n",
    "    pnt_hem, rot_mat = get_camera_pos(arena_pos, np.array([0, phi, theta]))\n",
    "    env.sim.data.cam_xpos[camera_id] = pnt_hem\n",
    "    env.sim.data.cam_xmat[camera_id] = rot_mat.flatten()\n",
    "\n",
    "    camera_obs = env.env.sim.render(\n",
    "        camera_name=env.env.camera_name,\n",
    "        width=env.env.camera_width,\n",
    "        height=env.env.camera_height,\n",
    "        depth=env.env.camera_depth,\n",
    "        #device_id=1,\n",
    "    )    \n",
    "    rgb, ddd = camera_obs\n",
    "\n",
    "    n_try = 0\n",
    "    while np.min(ddd) > 0.99:\n",
    "        camera_obs = env.env.sim.render(\n",
    "            camera_name=env.env.camera_name,\n",
    "            width=env.env.camera_width,\n",
    "            height=env.env.camera_height,\n",
    "            depth=env.env.camera_depth,\n",
    "            #device_id=1,\n",
    "        )    \n",
    "        rgb, ddd = camera_obs\n",
    "        n_try += 1\n",
    "        if n_try == 5:\n",
    "            return None\n",
    "\n",
    "    extent = env.env.mjpy_model.stat.extent\n",
    "    near = env.env.mjpy_model.vis.map.znear * extent\n",
    "    far = env.env.mjpy_model.vis.map.zfar * extent\n",
    "\n",
    "    im_depth = near / (1 - ddd * (1 - near / far)) \n",
    "    im_depth = np.flip(im_depth, axis=0)\n",
    "\n",
    "    return im_depth\n",
    "\n",
    "def _gqcnn_batch(depth_im, vis_on=True, num_candidates=10, num_vps=4):\n",
    "    camera_obs = env.env.sim.render(\n",
    "        camera_name=env.env.camera_name,\n",
    "        width=env.env.camera_width,\n",
    "        height=env.env.camera_height,\n",
    "        depth=env.env.camera_depth,\n",
    "        #device_id=1,\n",
    "    )    \n",
    "    rgb, ddd = camera_obs\n",
    "\n",
    "    n_try = 0\n",
    "    while np.min(ddd) > 0.99:\n",
    "        camera_obs = env.env.sim.render(\n",
    "            camera_name=env.env.camera_name,\n",
    "            width=env.env.camera_width,\n",
    "            height=env.env.camera_height,\n",
    "            depth=env.env.camera_depth,\n",
    "            #device_id=1,\n",
    "        )    \n",
    "        rgb, ddd = camera_obs\n",
    "        n_try += 1\n",
    "        if n_try == 5:\n",
    "            return None\n",
    "    # plt.imshow(depth_im)\n",
    "    # plt.show()\n",
    "\n",
    "    return env.env.policy.evaluate_4Dgqcnn_batch(np.flip(rgb, axis=0), depth_im, vis_on=vis_on, num_candidates=num_candidates, num_vps=num_vps)\n",
    "\n",
    "def viewpoint_quality_fn(thetas, phis, num_candidates=10, _vis_on=False): # ind_, queue_,\n",
    "    quality = np.zeros(len(thetas))\n",
    "\n",
    "    num_workers = 4 #mp.cpu_count()\n",
    "    depth_hats = np.zeros([len(thetas), 256, 256])\n",
    "    num_vp = len(thetas)\n",
    "    for i in range(num_vp):\n",
    "        depth_hats[i, :, :] = get_im_depth(thetas[i], phis[i])\n",
    "\n",
    "    num_batch = 4\n",
    "    for ind in range(0, int(num_vp / num_batch)): # when batch evaluation\n",
    "        trial_count = 0\n",
    "        results = None\n",
    "        d_im = None\n",
    "        brk = 0\n",
    "        while results is None and d_im is None:\n",
    "            if trial_count!=0:\n",
    "                print('GQ-CNN failed: #%d times.'%trial_count)\n",
    "            try:\n",
    "                results, depths, d_im = _gqcnn_batch(depth_hats[ind * num_batch:(ind + 1) * num_batch, :, :], vis_on=_vis_on, num_candidates=num_candidates, num_vps=num_batch) \n",
    "            except:\n",
    "                print('No Valid Grasps.')\n",
    "                pass\n",
    "\n",
    "            trial_count += 1\n",
    "            if trial_count == 3:\n",
    "                brk = 1\n",
    "                break\n",
    "        if brk == 1:\n",
    "            quality[ind * num_batch:(ind+1)*num_batch] = 0\n",
    "        else:\n",
    "            #HERE!\n",
    "            quality[ind * num_batch:(ind+1)*num_batch] = [result_.q_value for result_ in results] #results.q_value\n",
    "            #quality[ind * num_batch:(ind+1)*num_batch] = [np.mean([result.q_value for result in result_]) for result_ in results] #results.q_value\n",
    "    return quality\n",
    "\n",
    "def select_vp(num_iters=3, num_comp=3, num_seeds=100, num_gmm_samples=52, gmm_reg_covar=0.01, elite_p=0.25, num_candidates=10):\n",
    "    # initial uniformly random seeds\n",
    "    thetas  = np.random.uniform(0, 2 * np.pi, num_seeds)\n",
    "    phis = np.random.uniform(0, np.pi * 0.2, num_seeds)\n",
    "\n",
    "    for k in range(num_iters):\n",
    "        # evaluate and sort\n",
    "        num_vp = len(thetas)\n",
    "        qs = viewpoint_quality_fn(thetas=thetas, phis=phis, num_candidates=num_candidates, _vis_on=False)\n",
    "\n",
    "        print('Average Viewpoint Quality: ', np.mean(qs))\n",
    "        # cem_it_start = time.time()\n",
    "\n",
    "        # extract elites\n",
    "        q_values_and_indices = zip(qs, np.arange(num_vp))\n",
    "        q_values_and_indices = sorted(q_values_and_indices,\n",
    "                                      key=lambda x: x[0],\n",
    "                                      reverse=True)\n",
    "        num_elites = int(elite_p*num_vp)\n",
    "        elite_q_values = [i[0] for i in q_values_and_indices[:num_elites]]\n",
    "        elite_vp_indices = [i[1] for i in q_values_and_indices[:num_elites]]\n",
    "        elite_thetas = [thetas[i] for i in elite_vp_indices]\n",
    "        elite_phis = [phis[i] for i in elite_vp_indices]\n",
    "        elite_vps = np.array([ [theta,phi] for theta, phi in zip(elite_thetas,elite_phis) ])\n",
    "        # elite_grasp_arr = np.array([g.feature_vec for g in elite_grasps])\n",
    "\n",
    "        # Normalize elite set.\n",
    "        elite_vp_mean = np.mean(elite_vps, axis=0)\n",
    "        elite_vp_std = np.std(elite_vps, axis=0)\n",
    "        elite_vp_std[elite_vp_std == 0] = 1e-6\n",
    "        elite_vps = (elite_vps - elite_vp_mean) / elite_vp_std\n",
    "\n",
    "        # refit\n",
    "        uniform_weights = (1.0 / num_comp) * np.ones(num_comp)\n",
    "        gmm = GaussianMixture(n_components=num_comp,\n",
    "                                 weights_init=uniform_weights,\n",
    "                                 reg_covar=gmm_reg_covar)\n",
    "        gmm.fit(elite_vps)\n",
    "\n",
    "        # resample\n",
    "        vp_vecs, _ = gmm.sample(n_samples=num_gmm_samples)\n",
    "        vp_vecs = elite_vp_std * vp_vecs + elite_vp_mean\n",
    "        thetas = np.array([vp[0] for vp in vp_vecs]) \n",
    "        phis = np.array([vp[1] for vp in vp_vecs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fd01b471c453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcam_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15855\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcamera_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera_name2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eye_on_wrist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marena_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco_arena\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_abs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minit_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marena_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# - cam_offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "cam_offset = np.array([0.05, 0, 0.15855])\n",
    "camera_id = env.sim.model.camera_name2id(\"eye_on_wrist\")\n",
    "\n",
    "arena_pos = env.env.mujoco_arena.bin_abs\n",
    "init_pos = arena_pos + np.array([0, 0, 0.7])# - cam_offset\n",
    "release_pos = arena_pos + np.array([0, 0.59, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same procedures to the 1. 6DoF_data_collecting\n",
    "### Differences: sample the approaching vector using CEM method when test is False\n",
    "### get the approaching vector from the trained network when test is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset!\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Creating window glfw\n",
      "Place all objects in bin!\n"
     ]
    }
   ],
   "source": [
    "if test:\n",
    "    with open(config_file) as json_file:\n",
    "        config = json.load(json_file)\n",
    "    graspnet = VPPNET(config, model_dir=config[\"model_dir\"], is_training=False)\n",
    "\n",
    "for i in range(0, num_episodes):\n",
    "    print(\"Reset!\")\n",
    "    item_list, pos_list, quat_list = place_objects_in_bin(env, num_objects=env.num_objects)\n",
    "    print(\"Place all objects in bin!\")\n",
    "    vt_c_im_list, vt_d_im_list, rot_c_im_list, rot_d_im_list = [], [], [], []\n",
    "    vt_g_pos_list, vt_g_euler_list, rot_g_pos_list, rot_g_euler_list = [], [], [], []\n",
    "    g_label_list = []\n",
    "    success_count, failure_count, controller_failure = 0, 0, 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        stucked = move_to_pos_lrarm(env, np.array([0.4, 0.6, 1.0]), init_pos, level=1.0, render=render)\n",
    "        if stucked == -1:\n",
    "            controller_failure += 1\n",
    "            continue\n",
    "\n",
    "        vt_c_im, vt_d_im = get_vertical_image(env, arena_pos, vis_on=False)\n",
    "        if test:\n",
    "            prediction = graspnet.predict(vt_d_im.reshape((1, env.camera_width, env.camera_height, 1))[:, 80:176, 80:176, :])\n",
    "            weight = prediction[0]\n",
    "            max_weight = np.argmax(weight, axis=-1)\n",
    "            phi = (prediction[1][0, max_weight[0], 0] + 1.0) * np.pi / 10.0\n",
    "            theta = (prediction[1][0, max_weight[0], 1] + 1.0) * np.pi\n",
    "        else:\n",
    "            theta_targ, phi_targ, q_value = select_vp(num_iters=3, num_comp=3, num_seeds=36, num_gmm_samples=24, gmm_reg_covar=0.001, elite_p=0.25, num_candidates=1)\n",
    "            phi, theta = phi_targ, theta_targ\n",
    "\n",
    "        camera_obs = env.sim.render(\n",
    "            camera_name=env.camera_name,\n",
    "            width=env.camera_width,\n",
    "            height=env.camera_height,\n",
    "            depth=env.camera_depth\n",
    "        )\n",
    "        pnt_hem, rot_mat = get_camera_pos(arena_pos, np.array([0, phi, theta]))\n",
    "        env.sim.data.cam_xpos[camera_id] = pnt_hem\n",
    "        env.sim.data.cam_xmat[camera_id] = rot_mat.flatten()\n",
    "\n",
    "        # CEM best viewpoint\n",
    "        sel_start = time.time()\n",
    "\n",
    "        print(\"Try FC-GQ-CNN\")\n",
    "        [result, depths, d_im], rot_c_im, rot_d_im = env.env.gqcnn(vis_on=False, num_candidates=10) ## vis_on \n",
    "\n",
    "        if isinstance(result, Iterable):\n",
    "            sample_grasp_idx = np.random.randint(10, size=1)\n",
    "            result = result[sample_grasp_idx[0]].grasp\n",
    "        else:\n",
    "            if result is None:\n",
    "                continue\n",
    "            else:\n",
    "                result = result.grasp\n",
    "\n",
    "        p_x, p_y = result.center\n",
    "        print('p_x, p_y: ', p_x, p_y)\n",
    "\n",
    "        graspZ = result.depth\n",
    "        print('graspZ: ', graspZ)\n",
    "\n",
    "        dx, dy = env.env._pixel2pos(p_x, p_y, graspZ, arena_pos=[0, 0, 0])\n",
    "        dz = graspZ\n",
    "\n",
    "        t_angle = [result.angle, phi, theta]\n",
    "\n",
    "        #t_pos, _ = get_pos_and_rot(pnt_hem, 0, phi, theta, cam_offset=[-dx, dy, dz])\n",
    "\n",
    "        if test == False:\n",
    "            t_pos, _ = get_pos_and_rot(pnt_hem, 0, phi, theta, cam_offset=[-dx, dy, dz])\n",
    "            vt_c_im_list.append(vt_c_im)\n",
    "            vt_d_im_list.append(vt_d_im)\n",
    "            #rot_c_im_list.append(rot_c_im)\n",
    "            #rot_d_im_list.append(rot_d_im)\n",
    "            vt_g_pos_list.append(t_pos)\n",
    "            vt_g_euler_list.append(t_angle)\n",
    "\n",
    "            t_pos = pnt_hem\n",
    "            stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.10], grasp=0.0, level=4.0, render=render)\n",
    "            if stucked == -1:\n",
    "                controller_failure += 1\n",
    "                continue\n",
    "            print('arrived at the hover point')\n",
    "\n",
    "            # step 5. move to real target position\n",
    "            stucked = approach_to_object(env, t_pos, t_angle, cam_offset=[-dx, dy, dz + 0.01], grasp=0.0, level=2.0, render=render)\n",
    "            if stucked == -1:\n",
    "                failure_count += 1\n",
    "                g_label_list.append(0.0)\n",
    "                print('Grasping Failed, Success rate: ', success_count / (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                    ', total trial: ', success_count, failure_count, controller_failure)\n",
    "                stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.10], grasp=0.0, level=1.0, render=render)\n",
    "                continue\n",
    "\n",
    "            # step 6. grasping\n",
    "            obs = env.env._get_observation()\n",
    "            print('arrived at:', obs['right_eef_pos'], '(before grasping)')\n",
    "\n",
    "            stucked = force_gripper(env, grasp=1.0, render=render)\n",
    "            print(\"Try grasping!\")\n",
    "            stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.30], grasp=1.0, level=0.1, render=render)\n",
    "            print(\"Go back to the hover point.\")\n",
    "\n",
    "            # new step. collision check - success/failure\n",
    "            collision = env._check_contact()\n",
    "            if collision:\n",
    "                success_count += 1\n",
    "                g_label_list.append(1.0)\n",
    "                print('Grasping Success, Success rate: ', success_count / (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                    ', total trial: ', success_count, failure_count, controller_failure)\n",
    "                move_to_pos(env, release_pos, 0.0, 1.0, level=0.1, render=render)\n",
    "                force_gripper(env, grasp=0.0, render=render)\n",
    "            else:\n",
    "                failure_count += 1\n",
    "                g_label_list.append(0.0)\n",
    "                print('Grasping Failed, Success rate: ', success_count/ (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                    ', total trial: ', success_count, failure_count, controller_failure)\n",
    "\n",
    "            '''if (i + 1) % 10 == 0:\n",
    "                save_file_name = \"data/vpn_grasp/episode_\" + str(i) + \".npz\"\n",
    "                np.savez(save_file_name, vt_c_im_list, vt_d_im_list, vt_g_pos_list, vt_g_euler_list)'''\n",
    "        else:\n",
    "            t_pos = pnt_hem\n",
    "            stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.10], grasp=0.0, level=4.0, render=render)\n",
    "            if stucked == -1:\n",
    "                controller_failure += 1\n",
    "                continue\n",
    "            print('arrived at the hover point')\n",
    "\n",
    "            # step 5. move to real target position\n",
    "            stucked = approach_to_object(env, t_pos, t_angle, cam_offset=[-dx, dy, dz + 0.01], grasp=0.0, level=2.0, render=render)\n",
    "            if stucked == -1:\n",
    "                failure_count += 1\n",
    "                g_label_list.append(0.0)\n",
    "                print('Grasping Failed, Success rate: ', success_count / (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                    ', total trial: ', success_count, failure_count, controller_failure)\n",
    "                stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.10], grasp=0.0, level=1.0, render=render)\n",
    "                continue\n",
    "\n",
    "            # step 6. grasping\n",
    "            obs = env.env._get_observation()\n",
    "            print('arrived at:', obs['right_eef_pos'], '(before grasping)')\n",
    "\n",
    "            stucked = force_gripper(env, grasp=1.0, render=render)\n",
    "            print(\"Try grasping!\")\n",
    "            stucked = move_to_6Dpos(env, t_pos, t_angle, cam_offset=[-dx, dy, dz - 0.30], grasp=1.0, level=0.1, render=render)\n",
    "            print(\"Go back to the hover point.\")\n",
    "\n",
    "            # new step. collision check - success/failure\n",
    "            collision = env._check_contact()\n",
    "            if collision:\n",
    "                success_count += 1\n",
    "                g_label_list.append(1.0)\n",
    "                print('Grasping Success, Success rate: ', success_count / (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                    ', total trial: ', success_count, failure_count, controller_failure)\n",
    "                move_to_pos(env, release_pos, 0.0, 1.0, level=0.1, render=render)\n",
    "                force_gripper(env, grasp=0.0, render=render)\n",
    "            else:\n",
    "                failure_count += 1\n",
    "                g_label_list.append(0.0)\n",
    "                print('Grasping Failed, Success rate: ', success_count/ (success_count + failure_count + controller_failure), success_count / (success_count + failure_count), \n",
    "                    ', total trial: ', success_count, failure_count, controller_failure)\n",
    "\n",
    "    if test == False:\n",
    "        save_file_name = \"data/vpn_grasp/episode_\" + str(i) + \".npz\"\n",
    "        np.savez(save_file_name, vt_c_im_list, vt_d_im_list, vt_g_pos_list, vt_g_euler_list, g_label_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
